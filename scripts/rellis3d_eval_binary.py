#!/usr/bin/env python3
"""
Binary Traversability Prediction Evaluation on RELLIS-3D

This module evaluates binary traversability predictions (traversable/non-traversable)
against ground truth semantic ID annotations from the RELLIS-3D dataset. It computes
per-image and aggregate metrics including precision, recall, F1, IoU, and accuracy.

Main workflow:
1. Parse command-line arguments (index CSV, traversable IDs, output path)
2. Load prediction and ground truth masks from disk
3. Compute binary classification metrics per frame
4. Aggregate statistics and write outputs:
   - Per-image CSV with frame-level metrics
   - Summary JSON with micro (aggregated) and macro (per-image stats)
   - Summary TXT with human-readable results

Metrics computed:
  - Micro metrics: Aggregated from pixel-level confusion matrix
  - Macro metrics: Per-image mean/median with percentile distributions
  - Per-sequence statistics: Mean IoU per sequence

Example usage:
  python rellis3d_eval_binary.py \\
    --index_csv path/to/index.csv \\
    --traversable_ids "31,33,3" \\
    --out_csv results/metrics.csv
"""

import argparse, csv, json, math
from pathlib import Path
from typing import Iterable, Set, Tuple, Dict, Any
import numpy as np
import cv2


# ==============================================================================
# Utility Functions: ID Parsing and Mask Loading
# ==============================================================================

def parse_id_list(s: str) -> Set[int]:
    """
    Parse a string of comma-separated semantic IDs into a set.
    
    Supports formats: "31,33,3" or "[31,33,3]" (brackets are stripped)
    Empty strings return an empty set.

    Args:
        s (str): Comma-separated ID list (optionally wrapped in brackets)

    Returns:
        Set[int]: Set of parsed IDs
    """
    s = s.strip().strip("[]")
    if not s:
        return set()
    return {int(x.strip()) for x in s.split(",") if x.strip()}


def load_mask_pred(pred_path: Path) -> np.ndarray:
    """Load pred_traversable.png -> boolean mask (True=traversable)."""
    im = cv2.imread(str(pred_path), cv2.IMREAD_UNCHANGED)
    if im is None:
        raise FileNotFoundError(pred_path)
    if im.ndim == 3:
        im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    return im > 0


def load_mask_gt_id(gt_id_path: Path, trav_ids: Set[int]) -> np.ndarray:
    """Load ID mask (uint8/uint16) and binarize by trav_ids."""
    im = cv2.imread(str(gt_id_path), cv2.IMREAD_UNCHANGED)
    if im is None:
        raise FileNotFoundError(gt_id_path)
    if im.ndim == 3:
        # some exports might be colorized; try to recover single channel
        im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    # Supports uint8 or uint16 id maps
    out = np.zeros_like(im, dtype=bool)
    for tid in trav_ids:
        out |= im == tid
    return out


def metrics_from_masks(pred: np.ndarray, gt: np.ndarray) -> Dict[str, Any]:
    """Binary metrics with epsilon guards."""
    assert pred.shape == gt.shape
    pred = pred.astype(bool)
    gt = gt.astype(bool)
    tp = np.logical_and(pred, gt).sum()
    fp = np.logical_and(pred, np.logical_not(gt)).sum()
    fn = np.logical_and(np.logical_not(pred), gt).sum()
    tn = np.logical_and(np.logical_not(pred), np.logical_not(gt)).sum()
    eps = 1e-9
    precision = tp / (tp + fp + eps)
    recall = tp / (tp + fn + eps)
    f1 = 2 * precision * recall / (precision + recall + eps)
    iou = tp / (tp + fp + fn + eps)
    acc = (tp + tn) / (tp + tn + fp + fn + eps)
    pos_ratio = gt.sum() / (gt.size + eps)
    return dict(
        tp=int(tp),
        fp=int(fp),
        fn=int(fn),
        tn=int(tn),
        precision=float(precision),
        recall=float(recall),
        f1=float(f1),
        iou=float(iou),
        accuracy=float(acc),
        gt_pos_ratio=float(pos_ratio),
    )


def iter_index_rows(index_csv: Path) -> Iterable[Dict[str, str]]:
    """
    Iterate over rows in the index CSV generated by GSAM processing.
    
    The CSV should contain columns: sequence, frame, image_path, gt_id_path,
    pred_path, and status (e.g., "ok", "no_boxes", "no_valid_boxes").

    Args:
        index_csv (Path): Path to the index CSV file

    Yields:
        Dict[str, str]: Each row as a dictionary with column names as keys
    """
        r = csv.DictReader(f)
        for row in r:
            yield row


def _safe_div(a, b):
    """
    Safely compute a/b, returning 0.0 if b is zero (avoid division by zero).
    
    Args:
        a: Numerator (converted to float)
        b: Denominator (converted to float)
    
    Returns:
        float: a/b or 0.0 if b == 0
    """
    return float(a) / float(b) if float(b) != 0.0 else 0.0


def _percentile(values, q):
    """
    Calculate percentile of a list of values using linear interpolation.
    
    Uses the linear interpolation method (numpy's default) for percentile
    estimation between sorted values.

    Args:
        values (list): List of floats to compute percentile over
        q (float): Percentile in [0, 1] (e.g., 0.25 for 25th percentile)

    Returns:
        float: Interpolated percentile value (or 0.0 if values is empty)
    """
    if not values:
        return 0.0
    vs = sorted(values)
    k = (len(vs) - 1) * q
    f = math.floor(k)
    c = math.ceil(k)
    if f == c:
        return float(vs[int(k)])
    return float(vs[f] * (c - k) + vs[c] * (k - f))


def main():
    """
    Main evaluation pipeline.
    
    1. Parse arguments and validate traversable ID list
    2. Iterate through predictions and ground truth masks from index
    3. Compute per-image metrics
    4. Compute aggregate (micro and macro) statistics
    5. Write outputs to CSV, JSON, and TXT files
    
    Argument details:
      --index_csv: Output from rellis3d_gsam_singleprocess.py with paths
      --traversable_ids: Comma-separated semantic IDs (e.g., "31,33,3")
      --out_csv: Output metrics CSV (one row per image)
    """
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--index_csv",
        required=True,
        type=Path,
        help="Path to the index.csv created by GSAM run",
    )
    ap.add_argument(
        "--traversable_ids",
        required=True,
        type=str,
        help='Comma-separated list, e.g. "31,33,3"',
    )
    ap.add_argument(
        "--out_csv", required=True, type=Path, help="Per-image metrics CSV output"
    )
    args = ap.parse_args()

    trav_ids = parse_id_list(args.traversable_ids)
    if not trav_ids:
        raise SystemExit("No traversable IDs provided.")

    rows_out = []
    agg = dict(tp=0, fp=0, fn=0, tn=0)
    skipped = 0
    total = 0

    for row in iter_index_rows(args.index_csv):
        total += 1
        img_path = Path(row["image_path"])
        pred_rel = row["pred_path"]
        gt_id_path = row.get("gt_id_path") or ""
        status = row.get("status", "")

        # Build absolute paths
        pred_path = Path(args.index_csv.parent, pred_rel) if pred_rel else None
        gt_path = Path(gt_id_path) if gt_id_path else None

        # Skip if missing files
        if (
            (not pred_path)
            or (not pred_path.exists())
            or (not gt_path)
            or (not gt_path.exists())
        ):
            skipped += 1
            continue

        try:
            pred = load_mask_pred(pred_path)
            gt = load_mask_gt_id(gt_path, trav_ids)
            # If shapes differ, resize pred to gt
            if pred.shape != gt.shape:
                pred = cv2.resize(
                    pred.astype(np.uint8),
                    (gt.shape[1], gt.shape[0]),
                    interpolation=cv2.INTER_NEAREST,
                ).astype(bool)
            m = metrics_from_masks(pred, gt)
        except Exception as e:
            skipped += 1
            continue

        rows_out.append(
            {
                "sequence": row.get("sequence", ""),
                "frame": row.get("frame", ""),
                "image_path": str(img_path),
                "gt_id_path": str(gt_path),
                "pred_path": str(pred_path),
                "status": status,
                **m,
            }
        )

        # accumulate
        for k in ("tp", "fp", "fn", "tn"):
            agg[k] += m[k]

    # write per-image
    args.out_csv.parent.mkdir(parents=True, exist_ok=True)
    with args.out_csv.open("w", newline="") as f:
        cols = [
            "sequence",
            "frame",
            "image_path",
            "gt_id_path",
            "pred_path",
            "status",
            "tp",
            "fp",
            "fn",
            "tn",
            "precision",
            "recall",
            "f1",
            "iou",
            "accuracy",
            "gt_pos_ratio",
        ]
        w = csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows_out:
            w.writerow(r)

    # --- tiny aggregate summary (no pandas) --------------------------
    # Compute both micro (pixel-level aggregate) and macro (per-image statistics)
    out_dir = Path(args.out_csv).parent
    summary_txt = out_dir / "metrics_summary.txt"
    summary_json = out_dir / "metrics_summary.json"

    tot_tp = tot_fp = tot_fn = tot_tn = 0
    precisions, recalls, f1s, ious, accs = [], [], [], [], []
    per_seq_iou_sum, per_seq_cnt = {}, {}
    num_images = 0

    with open(args.out_csv, "r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            num_images += 1
            # Accumulate pixel-level counts for micro metrics (global TP, FP, FN, TN)
            tp = int(row["tp"])
            fp = int(row["fp"])
            fn = int(row["fn"])
            tn = int(row["tn"])
            tot_tp += tp
            tot_fp += fp
            tot_fn += fn
            tot_tn += tn

            # Collect per-image metrics for macro statistics (mean/median/percentiles)
            precisions.append(float(row["precision"]))
            recalls.append(float(row["recall"]))
            f1s.append(float(row["f1"]))
            iou = float(row["iou"])
            ious.append(iou)
            accs.append(float(row["accuracy"]))

            # Track per-sequence IoU for sequence-level summaries
            seq = row["sequence"]
            per_seq_iou_sum[seq] = per_seq_iou_sum.get(seq, 0.0) + iou
            per_seq_cnt[seq] = per_seq_cnt.get(seq, 0) + 1

    # Compute micro metrics from aggregated confusion matrix
    # (single global F1, IoU, etc. across all pixels)
    micro_precision = _safe_div(tot_tp, (tot_tp + tot_fp))
    micro_recall = _safe_div(tot_tp, (tot_tp + tot_fn))
    micro_f1 = _safe_div(
        2 * micro_precision * micro_recall, (micro_precision + micro_recall)
    )
    micro_iou = _safe_div(tot_tp, (tot_tp + tot_fp + tot_fn))
    micro_acc = _safe_div((tot_tp + tot_tn), (tot_tp + tot_fp + tot_fn + tot_tn))

    # macro (per-image) means/medians
    def _mean(xs):
        return sum(xs) / len(xs) if xs else 0.0

    def _median(xs):
        return _percentile(xs, 0.5)

    macro = {
        "precision_mean": _mean(precisions),
        "precision_median": _median(precisions),
        "recall_mean": _mean(recalls),
        "recall_median": _median(recalls),
        "f1_mean": _mean(f1s),
        "f1_median": _median(f1s),
        "iou_mean": _mean(ious),
        "iou_median": _median(ious),
        "accuracy_mean": _mean(accs),
        "accuracy_median": _median(accs),
        "iou_p25": _percentile(ious, 0.25),
        "iou_p75": _percentile(ious, 0.75),
    }

    per_sequence_mean_iou = {
        seq: _safe_div(per_seq_iou_sum[seq], per_seq_cnt[seq])
        for seq in sorted(per_seq_cnt)
    }

    summary = {
        "num_images": num_images,
        "micro": {
            "TP": tot_tp,
            "FP": tot_fp,
            "FN": tot_fn,
            "TN": tot_tn,
            "precision": micro_precision,
            "recall": micro_recall,
            "f1": micro_f1,
            "iou": micro_iou,
            "accuracy": micro_acc,
        },
        "macro": macro,
        "per_sequence_mean_iou": per_sequence_mean_iou,
    }

    # write files
    summary_json.write_text(json.dumps(summary, indent=2))
    summary_txt.write_text(
        "\n".join(
            [
                f"Images: {num_images}",
                f"MICRO (from totals): "
                f"TP={tot_tp} FP={tot_fp} FN={tot_fn} TN={tot_tn} | "
                f"Precision={micro_precision:.4f} Recall={micro_recall:.4f} "
                f"F1={micro_f1:.4f} IoU={micro_iou:.4f} Acc={micro_acc:.4f}",
                f"MACRO (per-image): "
                f"Precision μ/med={macro['precision_mean']:.4f}/{macro['precision_median']:.4f}  "
                f"Recall μ/med={macro['recall_mean']:.4f}/{macro['recall_median']:.4f}  "
                f"F1 μ/med={macro['f1_mean']:.4f}/{macro['f1_median']:.4f}  "
                f"IoU μ/med={macro['iou_mean']:.4f}/{macro['iou_median']:.4f}  "
                f"Acc μ/med={macro['accuracy_mean']:.4f}/{macro['accuracy_median']:.4f}",
                f"IoU percentiles: p25={macro['iou_p25']:.4f}  p50={macro['iou_median']:.4f}  p75={macro['iou_p75']:.4f}",
                "Per-sequence mean IoU:",
                *[
                    f"  {seq}: {per_sequence_mean_iou[seq]:.4f}"
                    for seq in per_sequence_mean_iou
                ],
            ]
        )
    )

    print(f"[*] Wrote {summary_txt}")
    print(f"[*] Wrote {summary_json}")
    # ---------------------------------------------------------------

    # Aggregate metrics summary (simple printf output)
    eps = 1e-9
    tp, fp, fn, tn = agg["tp"], agg["fp"], agg["fn"], agg["tn"]
    precision = tp / (tp + fp + eps)
    recall = tp / (tp + fn + eps)
    f1 = 2 * precision * recall / (precision + recall + eps)
    iou = tp / (tp + fp + fn + eps)
    acc = (tp + tn) / (tp + tn + fp + fn + eps)

    print("[*] Eval complete")
    print(f"    images total={total}  evaluated={len(rows_out)}  skipped={skipped}")
    print(f"    traversable_ids={sorted(trav_ids)}")
    print(f"    agg: TP={tp} FP={fp} FN={fn} TN={tn}")
    print(
        f"    Precision={precision:.4f}  Recall={recall:.4f}  F1={f1:.4f}  IoU={iou:.4f}  Acc={acc:.4f}"
    )


if __name__ == "__main__":
    main()
